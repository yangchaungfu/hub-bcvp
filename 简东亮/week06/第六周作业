导入 torch
导入 torch.nn 作为 nn
导入 torch.nn.functional 作为 F
导入数学


class  TransformerLayer ( nn . Module ):
    def  __init__ ( self , d_model = 512 , nhead = 8 , dim_feedforward = 2048 , dropout = 0.1 ):
        """
        单个 Transformer 层 - 简化实现
        参数：
            d_model: 特征维度
            nhead: 注意力头的数量
            dim_feedforward: 前馈网络的维度
            dropout：辍学概率
        """
        super ( TransformerLayer , self ).__ init__ ()
        self.d_model = d_model​​  
        self.nhead = nhead​​  
        self.dim_feedforward = dim_feedforward​​  

        #确保d_model可以被nhead整除
        assert  d_model  %  nhead  ==  0 , "d_model必须能被nhead整除"
        self.d_k = d_model // nhead # 每个头的维度

        # 自注意力的Q、K、V投影矩阵
        self.W_q = nn.Linear ( d_model , d_model , bias = False )​​ ​​ 
        self.W_k = nn.Linear  ( d_model , d_model , bias = False )​​​​ 
        self.W_v = nn.Linear ( d_model , d_model , bias = False )​​ ​​ 
        self.W_o = nn.Linear  ( d_model , d_model , bias = False )​​​​ 

        #前馈网络
        self.ffn = nn.Sequential (​​ ​​ 
            nn.Linear ( d_model , dim_feedforward ) ,​
            nn.ReLU()，
            nn.Dropout（dropout ），​
            nn.Linear ( dim_feedforward , d_model )​​
        ）

        #层归一化
        自己。范数 1  =  nn。层范数( d_model )
        自己。范数2  =  nn。层范数( d_model )

        ＃ 辍学
        self.dropout = nn.Dropout ( dropout )​​ ​​ 

    def  scaled_dot_product_attention ( self , Q , K , V , mask = None ):
        """
        缩放点积注意力计算过程
        """
        # 计算Q和K的点积
        scores  =  torch.matmul ( Q , K.transpose ( -2 , -1 ) )​​​​​

        # 缩放
        得分 = 得分 / math.sqrt(self.d_k)

        #申请掩码（如果有）
        如果 mask 不为 None：
            scores  =  scores.masked_fill ( mask == 0 , -1e9 )​​​  

        # 计算softmax获得焦点权重
        attn_weights  =  F.softmax ( scores , dim = -1 )​​​

        #应用dropout到注意力权重
        attn_weights  =  self.dropout ( attn_weights )​​

        # 乘以V得到输出
        输出 = torch.matmul(attn_weights, V)

        返回输出，attn_weights

    def  multi_head_attention ( self , x , mask = None ):
        """
        多头注意力计算过程
        """
        batch_size、seq_len、d_model  =  x.size ( )​

        # 线性投影得到Q、K、V
        Q  =  self.W_q ( x )​​
        K  =  self.W_k ( x )​​
        V  =  self.W_v ( x )​​

        # 成交为多头格式
        Q  =  Q.view ( batch_size , seq_len , self.nhead , self.d_k ) .transpose ( 1 , 2 )​​​​​​
        K  =  K.view ( batch_size , seq_len , self.nhead , self.d_k ) .transpose ( 1 , 2 )​​​​​​
        V  =  V.view ( batch_size , seq_len , self.nhead , self.d_k ) .transpose ( 1 , 2 )​​​​​​

        # 计算缩放点积注意力
        attn_output , attn_weights  =  self.scaled_dot_product_attention ( Q , K , V , mask )​​

        # 合并多头
        attn_output  =  attn_output.transpose ( 1 , 2 ) .contiguous ( ) . view (
            batch_size、seq_len、d_model
        ）

        # 输出投影
        输出 = self.W_o(attn_output)

        返回输出，attn_weights

    def  forward ( self , x , mask = None ):
        """
        前向传播
        """
        # 第一层归一化 (Pre-LN)
        x_norm1  =  self.norm1 ( x )​​

        #多头自我注意力
        attn_output , attn_weights  =  self.multi_head_attention ( x_norm1 , mask )​​

        # 注意力丢失和残差连接
        x  =  x  +  self.dropout ( attn_output )​​

        #第二个层归一化 (Pre-LN)
        x_norm2  =  self.norm2 ( x )​​

        #前馈网络
        ff_output  =  self.ffn ( x_norm2 )​​

        # 前馈网络丢失和残差连接
        x  =  x  +  self.dropout ( ff_output )​​

        返回 x，attn_weights


# 计算展示
如果 __name__ == "__main__":
    # 创建模型
    model  =  TransformerLayer ( d_model = 785 , nhead = 5 )

    # 输入数据
    batch_size  =  2
    序列长度 = 666
    input_tensor  =  torch.randn ( batch_size , seq_len , 785 )​​

    #创建因果间隙码
    mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), 对角线=1)

    #前向传播
    输出，attn_weights = model(input_tensor, mask)

    print ( f"输入形状: { input_tensor . shape } " )
    print ( f" 输出形状: {输出. shape } " )
    print ( f" 焦点权重形状: { attn_weights . shape } " )
