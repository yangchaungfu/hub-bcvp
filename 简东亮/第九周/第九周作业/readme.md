### 第九周 使用BERT进行NER任务
1. `loader.py`：直接使用bert模型的词表`vocab.txt`对输入文本进行逐字符编码，保证输入文本和输出标签序列等长
2 .  ` model.py ` : 将lstm模型层替换为bert模型层
3. `config.py`：训练配置参数，主要调节了`batch_size`、`learning_rate`和`use_crf`这3个参数
4. 运行结果：此次比赛结果展示4个不同参数配置下的bert模型训练结果：

|批量大小 |学习率 |使用_crf |宏-F1 |微F1 | 结果文件 |
| ------------ | --------------- | --------- | --------- | --------- | ------------------------ |
| 128 | 1e-4 | True | 0.874680 | 0.885872 | bert_with_crf_1e-4.log |
| 128 | 1e-4 | False |0.875741 |0.886359 | bert_only_1e-4.log |
| 32 | 1e-5 | True | 0.864031 | 0.876226 | bert_with_crf_1e-5.log |
| 32 | 1e-5 | False |0.859311 |0.874817 | bert_only_1e-5.log |
