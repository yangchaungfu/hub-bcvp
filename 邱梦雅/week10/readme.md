## Week10作业-BERT实现自回归语言模型训练
encoder类模型做自回归单向语言模型训练需要额外增加一个mask，保证前面的字看不到后面的字的信息；即同一模型，结构不变，通过特定形状的Mask Attention控制特定形式的训练
