# -*- coding: utf-8 -*-

import torch
import torch.nn as nn
from torch.optim import Adam, SGD

"""
å»ºç«‹ç½‘ç»œæ¨¡åž‹ç»“æž„
"""


class SentenceEncoder(nn.Module):
    def __init__(self, config):
        super(SentenceEncoder, self).__init__()
        embed_size = config["embed_size"]
        vocab_size = config["vocab_size"]
        sentence_len = config["sentence_len"]

        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        # self.lstm = nn.LSTM(embed_size, embed_size, batch_first=True, bidirectional=True)
        self.layer = nn.Linear(embed_size, embed_size)
        self.dropout = nn.Dropout(0.5)

    # è¾“å…¥ä¸ºé—®é¢˜å­—ç¬¦ç¼–ç 
    def forward(self, x):
        x = self.embedding(x)
        # ä½¿ç”¨lstm
        # x, _ = self.lstm(x)

        # shape: [batch_size, sentence_len, embed_size]
        x = self.layer(x)

        # [batch_size, sentence_len, embed_size] -> [batch_size, embed_size]
        return nn.functional.max_pool1d(x.transpose(1, 2), x.shape[1]).squeeze(-1)  # x.shape[1] è¡¨ç¤ºä»¥è¯­å¥é•¿åº¦ä½œä¸ºæ± åŒ–çª—å£


class SiameseNetwork(nn.Module):
    def __init__(self, config):
        super(SiameseNetwork, self).__init__()
        self.sentence_encoder = SentenceEncoder(config)
        self.loss = self.cosine_triplet_loss

    # è®¡ç®—ä½™å¼¦è·ç¦»  1-cos(a,b)
    # cos=1æ—¶ä¸¤ä¸ªå‘é‡ç›¸åŒï¼Œä½™å¼¦è·ç¦»ä¸º0ï¼›cos=0æ—¶ï¼Œä¸¤ä¸ªå‘é‡æ­£äº¤ï¼Œä½™å¼¦è·ç¦»ä¸º1
    def cosine_distance(self, tensor1, tensor2):
        """
        Args:
            tensor1: shape: [batch_size, embed_size]
            tensor2: shape: [batch_size, embed_size]
        Returns:
            shape: [batch_size]
        """
        # L2å½’ä¸€åŒ–ï¼Œå³ï¼šå°†-1ç»´çš„æ¯ä¸ªå…ƒç´  a[i]/L2èŒƒæ•° = a[i]/sqrt(sum(a[i]^2))
        tensor1 = torch.nn.functional.normalize(tensor1, dim=-1)
        tensor2 = torch.nn.functional.normalize(tensor2, dim=-1)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ axb/|a|*|b|ï¼Œå³ä¸¤ä¸ªçŸ©é˜µå½’ä¸€åŒ–åŽçš„å“ˆè¾¾çŽ›ç§¯
        cosine = torch.sum(torch.mul(tensor1, tensor2), dim=-1)
        return 1 - cosine

    #  ä¸‰å…ƒç»„æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼šð¿=max( d(a,p)-d(a,n)+margin, 0)ï¼Œ
    #  a: anchor åŽŸç‚¹ï¼Œp: positive ä¸ŽaåŒä¸€ç±»åˆ«çš„æ ·æœ¬ï¼Œn: negative ä¸Žaä¸åŒç±»åˆ«çš„æ ·æœ¬
    def cosine_triplet_loss(self, a, p, n, margin=None):
        """
        Args:
            a: åŽŸç‚¹ [batch_size, sen_len, embed_size]
            p: æ­£æ ·æœ¬ [batch_size, sen_len, embed_size]
            n: è´Ÿæ ·æœ¬ [batch_size, sen_len, embed_size]
            margin:
        """
        ap = self.cosine_distance(a, p)  # è®¡ç®—aå’Œpçš„ä½™å¼¦è·ç¦»
        an = self.cosine_distance(a, n)  # è®¡ç®—aå’Œnçš„ä½™å¼¦è·ç¦»
        # å¦‚æžœæ²¡æœ‰è®¾ç½®marginï¼Œåˆ™è®¾ç½®diffä¸ºap - an + 0.1
        if margin is None:
            diff = ap - an + 0.1  # [batch_size]
        # å¦‚æžœè®¾ç½®äº†marginï¼Œåˆ™è®¾ç½®diffä¸ºap - an + margin.squeeze(-1)
        else:
            diff = ap - an + margin.squeeze(-1)  # [batch_size]
        return torch.mean(torch.clamp(diff, min=0))  # å°†å°äºŽ0çš„å…ƒç´ è®¾ä¸º0ï¼Œå†è®¡ç®—è¿™æ‰¹lossçš„å¹³å‡å€¼

    def forward(self, sentence_a, sentence_p=None, sentence_n=None):
        # åŒæ—¶ä¼ å…¥ä¸‰ä¸ªå¥å­
        if sentence_n is not None:
            vector_a = self.sentence_encoder(sentence_a)  # [batch_size, sen_len] -> [batch_size, embed_size]
            vector_p = self.sentence_encoder(sentence_p)
            vector_n = self.sentence_encoder(sentence_n)
            return self.loss(vector_a, vector_p, vector_n)  # æ ‡é‡

        # å•ç‹¬ä¼ å…¥ä¸€ä¸ªå¥å­æ—¶ï¼Œè®¤ä¸ºæ­£åœ¨ä½¿ç”¨å‘é‡åŒ–èƒ½åŠ›
        elif sentence_p is None and sentence_n is None:
            return self.sentence_encoder(sentence_a)  # [batch_size, sen_len] -> [batch_size, embed_size]


def choose_optimizer(config, model):
    optimizer = config["optimizer"]
    learning_rate = config["learning_rate"]
    if optimizer == "adam":
        return Adam(model.parameters(), lr=learning_rate)
    elif optimizer == "sgd":
        return SGD(model.parameters(), lr=learning_rate)


if __name__ == "__main__":
    from config import Config

    Config["vocab_size"] = 10
    Config["sentence_len"] = 4
    model = SiameseNetwork(Config)
    s1 = torch.LongTensor([[1, 2, 3, 0], [2, 2, 0, 0]])
    s2 = torch.LongTensor([[1, 2, 3, 4], [3, 2, 3, 4]])
    l = torch.LongTensor([[1], [0]])
    y = model(s1, s2, l)
    print(y)
    # print(model.state_dict())
