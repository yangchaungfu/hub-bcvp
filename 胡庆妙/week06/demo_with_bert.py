import torch
import torch.nn as nn
from transformers import BertModel

'''
åŸºäºpytorchçš„è‡ªå®ç°çš„Bertæ¨¡å‹, å¹¶ä¸”æ‰©å±•äº†è®¡ç®—æ¨¡å‹å‚æ•°æ€»é‡çš„æ–¹æ³•
'''


class BertBaseTorch:
    # å°†é¢„è®­ç»ƒå¥½çš„æ•´ä¸ªæƒé‡å­—å…¸è¾“å…¥è¿›æ¥
    def __init__(self, state_dict, sen_len):
        self.hidden_size = 768  # è¯å‘é‡çš„ç»´åº¦ï¼Œä¸èƒ½æ”¹
        self.sen_len = sen_len
        self.num_attention_heads = 12  # Mulit-Headæœºåˆ¶è¦æ‹†åˆ†çš„headæ•°ï¼Œè¦è·Ÿé¢„è®­ç»ƒconfig.jsonæ–‡ä»¶ä¸­çš„num_attention_headsä¸€è‡´
        self.num_hidden_layers = 12  # è·Ÿé¢„è®­ç»ƒconfig.jsonæ–‡ä»¶ä¸­çš„num_hidden_layersä¸€è‡´

        self.load_weights(state_dict)

    def load_weights(self, state_dict):
        # embeddingå±‚çš„ 3ä¸ªembeddingçš„å‚æ•°
        self.word_embeddings = state_dict["embeddings.word_embeddings.weight"]  # è¯å‘é‡ç¼–ç , [è¯è¡¨å¤§å°, 768]
        self.segment_embeddings = state_dict["embeddings.token_type_embeddings.weight"]  # segmentç¼–ç , [2, 768]
        self.position_embeddings = state_dict["embeddings.position_embeddings.weight"]  # ä½ç½®ç¼–ç , [512, 768]

        # embeddingå±‚çš„ å½’ä¸€åŒ–å‚æ•°  [768, 768]
        self.embeddings_layer_norm_weight = state_dict["embeddings.LayerNorm.weight"]  # shape: [768]
        self.embeddings_layer_norm_bias = state_dict["embeddings.LayerNorm.bias"]  # shape: [768]

        # transformerå±‚ï¼ˆå¤šå±‚ï¼‰
        self.transformer_params = []
        for i in range(self.num_hidden_layers):
            # self-attentionçš„  Q,K,V -> softmax(Q * K.T) * V çš„å‚æ•°
            attention_q_w = state_dict["encoder.layer.%d.attention.self.query.weight" % i]  # shape: [768, 768]
            attention_q_b = state_dict["encoder.layer.%d.attention.self.query.bias" % i]  # shape: [768]
            attention_k_w = state_dict["encoder.layer.%d.attention.self.key.weight" % i]
            attention_k_b = state_dict["encoder.layer.%d.attention.self.key.bias" % i]
            attention_v_w = state_dict["encoder.layer.%d.attention.self.value.weight" % i]
            attention_v_b = state_dict["encoder.layer.%d.attention.self.value.bias" % i]

            # self-attentionçš„ Linear(Attention(ğ‘„,ğ¾,ğ‘‰)) çš„å‚æ•°
            attention_output_weight = state_dict[
                "encoder.layer.%d.attention.output.dense.weight" % i]  # shape: [768, 768]
            attention_output_bias = state_dict["encoder.layer.%d.attention.output.dense.bias" % i]  # shape: [768]

            # self-attentionçš„ å½’ä¸€åŒ–å±‚å‚æ•°
            attention_layer_norm_w = state_dict[
                "encoder.layer.%d.attention.output.LayerNorm.weight" % i]  # shape: [768]
            attention_layer_norm_b = state_dict["encoder.layer.%d.attention.output.LayerNorm.bias" % i]  # shape: [768]

            # feed forwardçš„ linar(gelu(linar(x)))çš„å‚æ•°
            ff_intermediate_weight = state_dict["encoder.layer.%d.intermediate.dense.weight" % i]  # shape: [3072,768]
            ff_intermediate_bias = state_dict["encoder.layer.%d.intermediate.dense.bias" % i]  # shape: [3072]
            ff_output_weight = state_dict["encoder.layer.%d.output.dense.weight" % i]  # shape: [768,3072]
            ff_output_bias = state_dict["encoder.layer.%d.output.dense.bias" % i]  # shape: [768]

            # feed forwardçš„ å½’ä¸€åŒ–å±‚çš„çš„å‚æ•°
            ff_layer_norm_w = state_dict["encoder.layer.%d.output.LayerNorm.weight" % i]  # shape: [768]
            ff_layer_norm_b = state_dict["encoder.layer.%d.output.LayerNorm.bias" % i]  # shape: [768]
            self.transformer_params.append(
                [attention_q_w, attention_q_b, attention_k_w, attention_k_b, attention_v_w, attention_v_b,
                 attention_output_weight, attention_output_bias,
                 attention_layer_norm_w, attention_layer_norm_b, ff_intermediate_weight, ff_intermediate_bias,
                 ff_output_weight, ff_output_bias, ff_layer_norm_w, ff_layer_norm_b])

        # poolerå±‚ tan(x*W.T + b) çš„å‚æ•°
        self.pooler_dense_weight = state_dict["pooler.dense.weight"]  # shape: [768, 768]
        self.pooler_dense_bias = state_dict["pooler.dense.bias"]  # shape: [768]

    # è®¡ç®—æ¨¡å‹çš„å‚æ•°æ€»é‡
    def cal_params_total(self):
        # embeddingå±‚çš„å‚æ•°
        ecnt = 0
        ecnt += self.cal_param_of_tensor(self.word_embeddings)  # è¯å‘é‡ç¼–ç , [è¯è¡¨å¤§å°, 768]
        ecnt += self.cal_param_of_tensor(self.segment_embeddings)  # segmentç¼–ç , [2, 768]
        ecnt += self.cal_param_of_tensor(self.position_embeddings)  # # ä½ç½®ç¼–ç , [512, 768]
        ecnt += self.cal_param_of_tensor(self.embeddings_layer_norm_weight)  # [768, 768]
        ecnt += self.cal_param_of_tensor(self.embeddings_layer_norm_bias)  # [768]
        print("\tembeddingå±‚çš„å‚æ•°é‡ï¼š", format(ecnt, ","))

        # transformerå±‚çš„å‚æ•°
        acnt = 0
        for item in self.transformer_params[0]:
            cnt = 1
            i = 0
            while (i < len(item.shape)):
                cnt *= item.shape[i]
                i += 1
            acnt += cnt
        acnt *= self.num_hidden_layers
        print("\ttransformerå±‚çš„å‚æ•°é‡ï¼š", format(acnt, ","))

        # poolerå±‚ tan(x*W.T + b) çš„å‚æ•°
        pcnt = 0
        pcnt += self.cal_param_of_tensor(self.pooler_dense_weight)  # [768, 768]
        pcnt += self.cal_param_of_tensor(self.pooler_dense_bias)  # [768]
        print("\tpoolerå±‚çš„å‚æ•°é‡ï¼š", format(pcnt, ","))

        return ecnt + acnt + pcnt

    # è®¡ç®—æŒ‡å®šå¼ é‡åŒ…å«å‚æ•°
    def cal_param_of_tensor(self, tensor):
        cnt = 1
        i = 0
        while (i < len(tensor.shape)):
            cnt *= tensor.shape[i]
            i += 1
        return cnt

    # æœ€ç»ˆè¾“å‡º
    def forward(self, x):
        print("\n1. embeddingå±‚è¾“å…¥:\n", x)
        # [batch_size, sen_len] -> [batch_size, sen_len, 768]
        embeded_x = self.embedding_layer(x)
        # print("  embeddingå±‚è¾“å‡ºï¼š\n", embeded_x)

        print("\n2. transformerå±‚è¾“å…¥:\n", embeded_x)
        sequence_output = self.transformer_layers(embeded_x)
        # print("  transformerå±‚è¾“å‡ºï¼š\n", sequence_output)

        # sequence_output[0] è¡¨ç¤º?
        print("\n3. pooler_outputå±‚è¾“å…¥:\n", sequence_output)
        # shape: [batch_size, sen_len, 768) ->  [batch_size, 768]
        pooler_output = self.pooler_output_layer(sequence_output)
        return sequence_output, pooler_output

    # bert embeddingï¼Œä½¿ç”¨3å±‚å åŠ ï¼Œåœ¨ç»è¿‡ä¸€ä¸ªLayer normå±‚
    def embedding_layer(self, x):
        batch_size = x.shape[0]

        print("\t1.1. å¯¹xåš word embedding, segment embedding, position embedding ...")
        # [batch_size, sen_len] -> [batch_size, sen_len, 768]
        we = self.get_embedding(self.word_embeddings, x)

        # [batch_size, sen_len] -> [batch_size, sen_len, 768]
        te = self.get_embedding(self.segment_embeddings,
                                torch.LongTensor(batch_size * [self.sen_len * [0]]))

        # [batch_size, sen_len] -> [batch_size, sen_len, 768]
        pe = self.get_embedding(self.position_embeddings,
                                torch.LongTensor(batch_size * [list(range(self.sen_len))]))

        print("\t1.2. å°†è¿™ä¸‰ç§embeddingç›¸åŠ ï¼Œå†åš layer norm ...")
        embedding = we + pe + te

        # [batch_size, sen_len, 768]
        return self.layer_norm(embedding, self.embeddings_layer_norm_weight, self.embeddings_layer_norm_bias)

    def get_embedding(self, embedding_matrix, x):
        batch_size = x.shape[0]
        vectors = []
        for i in range(batch_size):
            vectors.append(embedding_matrix[x[i]])
        result = torch.stack(vectors, dim=0).squeeze(1)  # æˆ–è€…ç›´æ¥ dim=0
        return result

    def layer_norm(self, x, layer_norm_w, layer_norm_b):
        batch_size = x.shape[0]
        vectors = []
        for i in range(batch_size):
            mean = torch.mean(x[i], dim=1, keepdim=True)
            std = torch.std(x[i], dim=1, keepdim=True)
            vectors.append((x[i] - mean) / std)
        norm_x = torch.stack(vectors, dim=0).squeeze(1)
        return norm_x * layer_norm_w + layer_norm_b

    # æ‰§è¡Œå…¨éƒ¨çš„transformerå±‚è®¡ç®—
    def transformer_layers(self, x):
        """
        Args:
            x: shape: [batch_size, sen_len, 768]
        """
        for i in range(self.num_hidden_layers):
            print(f"\ttransformer ç¬¬{i + 1}å±‚ çš„è®¡ç®—...")
            x = self.transformer_layer(x, i)
        return x

    # æ‰§è¡Œå•å±‚transformerå±‚è®¡ç®—
    def transformer_layer(self, embedding_x, layer_index):
        """
        Args:
            embedding_x: shape: [batch_size, sen_len, 768]
        Returns:
            è¿”å›å‚çš„ shape: [batch_size, sen_len, 768]
        """
        params = self.transformer_params[layer_index]

        # å–å‡ºè¯¥å±‚çš„å‚æ•°ï¼Œåœ¨å®é™…ä¸­ï¼Œè¿™äº›å‚æ•°éƒ½æ˜¯éšæœºåˆå§‹åŒ–ï¼Œä¹‹åè¿›è¡Œé¢„è®­ç»ƒ
        attention_q_w, attention_q_b, \
            attention_k_w, attention_k_b, \
            attention_v_w, attention_v_b, \
            attention_output_weight, attention_output_bias, \
            attention_layer_norm_w, attention_layer_norm_b, \
            ff_intermediate_weight, ff_intermediate_bias, \
            ff_output_weight, ff_output_bias, \
            ff_layer_norm_w, ff_layer_norm_b = params

        # self attention çš„è®¡ç®—
        print("\t\t2.1. self attention çš„è®¡ç®— ... ")
        attention_x = self.self_attention(embedding_x,
                                          attention_q_w, attention_q_b,
                                          attention_k_w, attention_k_b,
                                          attention_v_w, attention_v_b,
                                          attention_output_weight,
                                          attention_output_bias)

        print("\t\t2.2. ä½¿ç”¨æ®‹å·®æœºåˆ¶(å³ï¼šembedding_x + attention_x), å†åšlayer norm ... ")
        # shape: [batch_size, sen_len, 768] * [768] + [768] -> [batch_size, sen_len, 768]
        attention_normed_x = self.layer_norm(embedding_x + attention_x, attention_layer_norm_w, attention_layer_norm_b)

        # feed forwardå±‚
        print("\t\t2.3. feed forward çš„è®¡ç®— ... ")
        feed_forward_x = self.feed_forward(attention_normed_x,
                                           ff_intermediate_weight, ff_intermediate_bias,
                                           ff_output_weight, ff_output_bias)

        print("\t\t2.4. ä½¿ç”¨æ®‹å·®æœºåˆ¶(å³ï¼šattention_normed_x + feed_forward_x), å†åšlayer norm ... ")
        return self.layer_norm(attention_normed_x + feed_forward_x, ff_layer_norm_w, ff_layer_norm_b)

    def self_attention(self,
                       embeded_x,
                       attention_q_w,
                       attention_q_b,
                       attention_k_w,
                       attention_k_b,
                       attention_v_w,
                       attention_v_b,
                       attention_output_weight,
                       attention_output_bias):
        """
        Args:
            embeded_x: shap: [batch_size, sen_len, 768]
        Returns:
            è¿”å›å‚çš„shape: [batch_size, sen_len, 768]
        """
        batch_size = embeded_x.shape[0]
        attention_head_size = int(self.hidden_size / self.num_attention_heads)  # Muliti-Headæœºåˆ¶æ¯ä¸ªHeadçš„åˆ—æ•°

        attention_vectors = []
        for i in range(batch_size):
            print(f"\t\t\tç¬¬ {i} ä¸ªæ‰¹é‡...")
            print("\t\t\t\tè®¡ç®— q, k, vï¼Œ q=linear(x), k=linear(x), v=linear(x) ...")
            # shape: [sen_len, 768] * [768, 768] + [768] -> [sen_len, 768]
            q = torch.matmul(embeded_x[i], attention_q_w.T) + attention_q_b
            k = torch.matmul(embeded_x[i], attention_k_w.T) + attention_k_b
            v = torch.matmul(embeded_x[i], attention_v_w.T) + attention_v_b

            # æ‹†åˆ† Muliti-Head, shape: [sen_len, 768] -> [sen_len, 12, 64] -> [12, sen_len, 64]
            print("\t\t\t\tå°† q, k, v æ‹†åˆ†ä¸ºå¤šå¤´ï¼Œç›¸å½“äº: [sen_len, 768] -> [12, sen_len, 768/12] ... ")
            q = q.reshape(self.sen_len, self.num_attention_heads, attention_head_size)
            q = q.transpose(0, 1)
            k = k.reshape(self.sen_len, self.num_attention_heads, attention_head_size)
            k = k.transpose(0, 1)
            v = v.reshape(self.sen_len, self.num_attention_heads, attention_head_size)
            v = v.transpose(0, 1)

            # è®¡ç®— softmax(q * k.T/sqrt(head_size)) * v
            print("\t\t\t\tè®¡ç®— qkv = softmax(q * k.T/sqrt(head_size)) * v ...")
            # q * k.T,  shape: [12, sen_len, 64] * [12, 64, sen_len] -> [12, sen_len, sen_len]
            qk = torch.matmul(q, k.transpose(1, 2))
            qk = torch.softmax(qk / torch.sqrt(torch.LongTensor([attention_head_size])), dim=-1)
            # shape: [12, sen_len, sen_len] * [12, sen_len, 64] -> [12, sen_len, 64]
            qkv = torch.matmul(qk, v)

            # shape: [12, sen_len, 64] -> [sen_len, 12, 64] -> [sen_len, 768]
            qkv = qkv.transpose(0, 1).reshape(-1, self.hidden_size)

            print("\t\t\t\tè®¡ç®— attentionçš„è¾“å‡ºï¼šlinear( attention(k,q,v) ) ...")
            # shape: [sen_len, 768] * [768, 768] -> [sen_len, 768]
            attention = torch.matmul(qkv, attention_output_weight.T) + attention_output_bias
            attention_vectors.append(attention)

        return torch.stack(attention_vectors, dim=0)  # shape: [batch_size, sen_len, 768]

    # å‰é¦ˆç½‘ç»œçš„è®¡ç®—
    def feed_forward(self,
                     attention_normed_x,
                     intermediate_weight,
                     intermediate_bias,
                     output_weight,
                     output_bias,
                     ):
        """
        Args:
            attention_normed_x: shap: [batch_size, sen_len, 768]
        Returns:
            è¿”å›å‚çš„shape: [batch_size, sen_len, 768]
        """
        batch_size = attention_normed_x.shape[0]
        sequence_vectors = []
        for i in range(batch_size):
            print(f"\t\t\tç¬¬ {i} ä¸ªæ‰¹é‡...")
            print("\t\t\t\tè®¡ç®— linear(gelu(linear(x))) ...")
            # shape: [sen_len, 768] * [768, 3072] + [3072] -> [sen_len, 3072]
            tmp = torch.matmul(attention_normed_x[i], intermediate_weight.T) + intermediate_bias

            tmp = nn.GELU()(tmp)

            # shape: [sen_len, 3072] * [3072, 768] + [768] -> [sen_len, 768]
            tmp = torch.matmul(tmp, output_weight.T) + output_bias
            sequence_vectors.append(tmp)
        return torch.stack(sequence_vectors, dim=0)  # shape: [batch_size, sen_len, 768]

    def pooler_output_layer(self, x):
        """
        Args:
            x: shape: [batch_size, sen_len, 768)
        Returns:
            è¿”å›å‚çš„shape: [batch_size, 768]
        """
        batch_size = x.shape[0]

        pooler_vectors = x[:, 0, :]  # å–æ¯æ¬¡æ‰¹æ¬¡ä¸­æ¯å¥è¯çš„æ¯ä¸€ä¸ªtokenå¯¹åº”çš„å‘é‡
        print("ç»“æœå½¢çŠ¶:", pooler_vectors.shape)  # torch.Size([batch_size, 768])

        # shape: [batch_size, 768] * [768, 768] + [768] -> [batch_size, 768]
        poller_output = torch.matmul(pooler_vectors, self.pooler_dense_weight.T) + self.pooler_dense_bias
        poller_output = torch.tanh(poller_output)
        return poller_output


x = torch.LongTensor([[2450, 15486, 102, 2110], [2450, 15486, 102, 2110]])  # å‡æƒ³æˆ4ä¸ªå­—çš„å¥å­

bert = BertModel.from_pretrained(r"D:\Miniconda3\bert-base-chinese", return_dict=False)
state_dict = bert.state_dict()
bert.eval()

sequence_output, pooler_output = bert(x)
print("\nsequence_output:\n", sequence_output, sequence_output.shape)  # shape: [batch_size, sen_len, 768]
print("pooler_output:\n", pooler_output, pooler_output.shape)  # shape: [batch_size, 768]
# print(bert.state_dict().keys())  # æŸ¥çœ‹æ‰€æœ‰çš„å‚æ•°çš„åç§°

mybert = BertBaseTorch(state_dict, 4)
my_sequence_output, my_pooler_output = mybert.forward(x)
print("my_sequence_output: \n", my_sequence_output, my_sequence_output.shape)  # shape: [batch_size, sen_len, 768]
print("my_pooler_output: \n", my_pooler_output, my_pooler_output.shape)  # shape: [batch_size, 768]

print("è®¡ç®—æ¨¡å‹å‚æ•°æ€»é‡...")
total_params_cnt = mybert.cal_params_total()
print("æ¨¡å‹å‚æ•°æ€»é‡: ", format(total_params_cnt, ","))
